{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of fraudulent credit card transactions using PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-338a2d4d563b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshuffle\u001b[0m         \u001b[1;31m# for shuffling our datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m    \u001b[1;31m# for plotting the training process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m                       \u001b[1;31m# deep learning framework\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd                # for reading csv data\n",
    "import numpy as np                 # for efficient matrix multiplication\n",
    "from random import shuffle         # for shuffling our datasets\n",
    "import matplotlib.pyplot as plt    # for plotting the training process\n",
    "import torch                       # deep learning framework\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and read the dataset\n",
    "You can download the dataset from https://www.kaggle.com/mlg-ulb/creditcardfraud/downloads/creditcard.csv/3\n",
    "\n",
    "This dataset constists of 284807 examples of credit card transactions. Each example has 28 features as well as the time (to the nearest hour) which it occured. The first column is the time (to the nearest hour) of the transaction, the remaining features are  The last column is the label of whether the transaction was fraudulent (1) or not (0) - this can be interpreted as the true probability of fraud, which is what a perfect model that we will make should predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('creditcard.csv')     # read the data from a spreadsheet\n",
    "data = np.array(data)                    # convert to matrix so we can use it in the math\n",
    "\n",
    "#data = data[:, :]                             # you can slice the matrix to only use certain features or examples\n",
    "'''\n",
    "labels = data[:, -1]         # the labels are the last column (0 for legitimate or 1 for fraudulent)\n",
    "features = data[:, :-1]      # crop off the indices and labels\n",
    "print(features.shape)        # show the shape of the features\n",
    "print(labels.shape)          # show the shape of the labels\n",
    "'''\n",
    "print(data)                  # show the data\n",
    "print(data.shape)            # show the shape of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What the hell are these features?\n",
    "\n",
    "The features of this dataset are not the raw features that we might collect about a card transaction, like how many tries it took to get the PIN right, or how quickly the transaction was made etc.\n",
    "Instead, they are actually a transformation of those original features, such that the new axes point in the direction in which the features vary the most. The new axes are still orthogonal to one another (the transformation is just a rotation of the axes). Transforming the features in this way is called principal component analysis (PCA). Performing PCA should mean that the first few features represent most of the variation of the data, and hence are the most important. This means that we could probably get decent results using only those first few features. This is a form of 'dimensionality reduction'.\n",
    "\n",
    "This isn't the main focus of the session, but is the explanation for what the features represent, and their strange names (V1, V2... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the dataset unbiased\n",
    "\n",
    "Most credit card transactions aren't fraudulent. As such, there are far fewer examples of fraudulent transactions than there are legitimate ones. If we train our model on all of these examples then it will be able to achieve a 99.83% accuracy by just classifying every example as legitimate! \n",
    "\n",
    "In order to counteract the bias of the dataset, we will adjust it to make it contain an even number of examples from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[:, -1]                # binary vector which has 1s where there is a fraud and 0s otherwise\n",
    "n_fraud = np.sum(labels)            # sum of the labels is the number of fraudulent examples\n",
    "\n",
    "# print the stats\n",
    "print('Number of fraudulent transactions:', n_fraud)\n",
    "print('Number of legitimate transactions:', len(data) - n_fraud)\n",
    "print('Percentage of examples that are fraudulent {:.2f}%:'.format((n_fraud / len(data))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "\n",
    "fraud_indices = data[:, -1] == 1              # get a boolean vector which has TRUE at the index of each fraudulent \n",
    "print(fraud_indices)                      \n",
    "fraud_examples = data[fraud_indices]      # index the data with the binary vector to get the fraudulent examples\n",
    "\n",
    "legit_indices = ~fraud_indices            # the tilde (~) inverses the binary vector\n",
    "print(legit_indices)\n",
    "legit_examples = data[legit_indices]      # index the data to get the legit examples\n",
    "clipped_legit_examples = legit_examples[:len(fraud_examples)]       # clip the legit examples so that we have the same number of examples with each label\n",
    "print(legit_examples.shape)\n",
    "\n",
    "data = np.vstack((fraud_examples, clipped_legit_examples))    # vertically stack the fraudulent and legit examples into a dataset where there are an even number of examples from each class\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CENTER AROUND MEAN\n",
    "# we dont want to normalise the labels! so separate them from the features \n",
    "features = data[:, :-1]\n",
    "labels = data[:, -1]\n",
    "\n",
    "features -= features.mean(axis=0)       # subtract the mean of each feature (over all rows axis=0) from each feature\n",
    "\n",
    "# DIVIDE BY RANGE\n",
    "max_features = features.max(axis=0)       # find the larget value of each feature\n",
    "min_features = features.min(axis=0)       # find the smallest value of each feature\n",
    "\n",
    "ranges = max_features - min_features      # find the range of each feature\n",
    "\n",
    "features /= ranges      # divide by range\n",
    "print(features)\n",
    "\n",
    "print(features.shape)\n",
    "labels = np.reshape(labels, (features.shape[0], 1))       # turn labels from a m-vector into a mx1 matrix so that we can stack\n",
    "print(labels.shape)\n",
    "\n",
    "normalised_data = np.hstack((features, labels))     # horizontally stack labels back onto the normalised features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into training, validation and test sets\n",
    "\n",
    "It's no use training our model to just perform well on the data we show it. It needs to be able to perform well on unseen examples.\n",
    "\n",
    "These unseen examples will come from a part of our dataset that we break off into a 'test set'.\n",
    "\n",
    "For less obvious reasons, we also need to create a 'validation set'. We will see that there are some design choices of our model (hyperparameters, rather than model parameters) that we shouldn't learn from the training set; and if we adjust these based on the model's performance on the test set, then we are training the hyperparameters on the test set... and then performance on the test set no longer becomes representative of performance on unseen examples! The point of the validation set is to train these hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(normalised_data))\n",
    "val_size = int(0.15 * len(normalised_data))\n",
    "test_size = len(normalised_data) - train_size - val_size\n",
    "train_data, val_data, test_data = random_split(normalised_data, [train_size, val_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up hyperparameters\n",
    "\n",
    "Now we have sorted out the data, we are ready to start building the rest of the model.\n",
    "\n",
    "Firstly we will set some hyperparameters.\n",
    "\n",
    "Hyperparameters are different to parameters because it doesn't make sense to learn them during training. Some examples include:\n",
    "- the depth of our model\n",
    "- the width of our model\n",
    "- batch size (how many examples we show the model at once)\n",
    "- learning rate (how much we change our parameters by on each update)\n",
    "- epochs to train for (how many times we pass the whole dataset through our model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16          # how many examples will we pass through our model at once\n",
    "lr = 0.001               # how big will the step sizes of our model parameter updates be\n",
    "momentum = 0.6           # what proportion of the previous parameter update will also contribute to the next\n",
    "epochs = 1               # how many times will we pass our whole dataset through the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the data loaders\n",
    "\n",
    "Now we have the dataset, we will create something to pass us the data in mini-batches and shuffle if for us - a data loader.\n",
    "\n",
    "We want to pass our data to our model in mini-batches (rather than the whole batch at once) for 2 main reasons:\n",
    "- passing the whole batch through the model will take longer, and slow each training step\n",
    "- How badly the model performs is a function of the data we pass through it. If we update our model parameters based on what will improve predictions for the batch as a whole, we may actually end up not optimisig for any of them specifically.\n",
    "\n",
    "This is an implementation from scratch. In the regression example we will use a pre-built class that PyTorch provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():                           # create a data loader to pass our examples to us in batches\n",
    "    \n",
    "    def __init__(self, dataset, batch_size):     # what happens when we create an dataloader instance\n",
    "        # MAKE BATCHES OF DATA\n",
    "        self.batches = []                        # initialisee empty list of batches\n",
    "        i = 0                                    # initial index to count where we are counting batches from\n",
    "        while i + batch_size < len(dataset):     # before we reach the end of the dataset\n",
    "            self.batches.append(dataset[i:i+batch_size])        # grab a batch from the data and append it to the list of batches\n",
    "            i += batch_size                      # increase the index to start at the next batch\n",
    "        self.batches.append(dataset[i:])         # the last batch may not fit into the \n",
    "        shuffle(self.batches)                    # shuffle the batches\n",
    "        \n",
    "    def __getitem__(self, idx):           # this function is called when we index the data loader e.g. dataset[4]\n",
    "        if idx == 0:  \n",
    "            shuffle(self.batches)                # shuffle the batches each epoch\n",
    "        batch = self.batches[idx]                # get a batch of examples from the list of batches\n",
    "        features = batch[:, :-1]                 # get the features from that batch (all rows and all columns up until the last one)\n",
    "        features = torch.tensor(features)        # turn the features into a torch tensor\n",
    "        features = features.float()              # change the data type\n",
    "        labels = batch[:, -1]                    # get the labels from the batch (all rows, last column)\n",
    "        labels = torch.tensor(labels)           \n",
    "        labels = labels.float()\n",
    "        labels = labels.unsqueeze(1)             # change from vector to matrix (so the labels come out as the same size as our predictions)\n",
    "        return features, labels                  # return the features and labels\n",
    "\n",
    "# CREATE A DATA LOADER\n",
    "train_loader = DataLoader(train_data, batch_size)          # create a data loader from the normalised dataset of a certain batch size\n",
    "val_loader = DataLoader(val_data, batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size)\n",
    "\n",
    "\n",
    "# SHOW AN EXAMPLE OF A BATCH PRODUCED\n",
    "print(train_loader[1])\n",
    "x, y = train_loader[1]\n",
    "print(x.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model\n",
    "\n",
    "We are going to create a function to map our inputs to our output (confidence of transaction being false)\n",
    "\n",
    "We call the function that we will create to perform this mapping a model, because it should model some ideal function that really maps these types of inputs to these outputs.\n",
    "\n",
    "We build a neural network of adjustable width and depth, that takes in a vector the size of our inputs and outputs a scalar probabiility of an example being fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE SIZES FOR EACH OF OUT NEURAL NETWORKS LAYERS\n",
    "units1 = 16\n",
    "units2 = 16\n",
    "units3 = 16\n",
    "\n",
    "class NN(torch.nn.Module):                                   # create a neural network class\n",
    "    \n",
    "    def __init__(self, n_features=30, n_outputs=1):          # what happens when we create a neural network instance\n",
    "        super().__init__()                                   # initialise the parent class\n",
    "        # DEFINE LAYERS TO TAKE FEATURES TO A PROBABILITY OF THIS EXAMPLE BEING FRAUDULENT\n",
    "        self.layers = torch.nn.Sequential(                   # define the layers of the model sequentially\n",
    "            torch.nn.Linear(n_features, units1),             # linear layers form weighted combinations of their inputs\n",
    "            torch.nn.ReLU(),                                 # Rectified Linear Unit activation function\n",
    "            torch.nn.Linear(units1, units2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units2, units3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units3, units3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units3, units3),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(units3, 1),\n",
    "            torch.nn.Sigmoid()   # sigmoid function squashes our output in the range 0-1, so it can be a probability\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):          # this function is called when we call our model with some input e.g. model(x)\n",
    "        x = self.layers(x)         # pass the features of the example through the layers of our model\n",
    "        return x                   # return the transformed output (should tell us whether we predict fraud or not)\n",
    "    \n",
    "nn = NN()           # now actually create an instance of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the loss function\n",
    "\n",
    "The loss function is a measure of how badly the model is currently performing. To measure this when our labels are labels are binary (either 0 or 1) we can use the binary cross entropy loss.\n",
    "\n",
    "When the label is y=1 (fraudulent) the second term is multiplied by 0 (1-y) and only the first term contributes to the loss. $log(\\hat{y})$ increases as the prediction ($\\hat{y}$) moves away from 1.\n",
    "\n",
    "When the label is y=0 (legitimate) the first term is multiplied by 0 (y) and only the second term contributes to the loss. $log(1 - \\hat{y})$ increases as the prediction ($\\hat{y}$) moves away from 0.\n",
    "\n",
    "# $L = - [ \\ y \\ log(\\hat{y}) + (1-y) \\ log(1 - \\hat{y}) \\ ]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()           # binary cross entropy loss function. Can be called to return loss between prediction and label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the optimiser\n",
    "\n",
    "The optimiser will update the parameters (weights and biases) of the model in a direction that reduces the error.\n",
    "\n",
    "We will use stochastic gradient descent (SGD), which updates the weights based on the update rule: \n",
    "# $w \\leftarrow w - \\alpha \\frac{\\partial L}{\\partial w}$ \n",
    "\n",
    "This means that the weights are moved in the direction that decreases the loss. The step size is proportional to the gradient by the learning rate (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = torch.optim.Adam(nn.parameters(), lr=lr)#, momentum=momentum)    # define how we will update our weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Here we repeatedly pass mini-batches of examples through our model, then compare our output to the corresponding labels and update the model weights based on the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=10):                 # define a training function\n",
    "    train_losses = []                        # initialise an empty list of the losses\n",
    "    \n",
    "    batch_idx_for_val = 0\n",
    "    validation_batch_idxs = []\n",
    "    validation_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):              # for however many epochs we specified\n",
    "        # TRAINING\n",
    "        print('Training')\n",
    "        for batch_idx, batch in enumerate(train_loader):     # for each batch in the training data loader\n",
    "            features, labels = batch                   # unpack the batch\n",
    "            print(labels)\n",
    "            prediction = model(features)               # pass an example's features forward through the model\n",
    "            print(prediction)\n",
    "            loss = criterion(prediction, labels)       # calculate the loss\n",
    "            optimiser.zero_grad()                      # zero the gradients (otherwise they will accumulate)\n",
    "            loss.backward()                            # find rate of change of loss with respect to model params\n",
    "            optimiser.step()                           # update the weights of our model\n",
    "            print('Epoch:', epoch, '\\tBatch:', batch_idx, '\\tLoss:', loss.item())\n",
    "            train_losses.append(loss.item())           # add this loss to the list of losses\n",
    "            \n",
    "            batch_idx_for_val += 1 # counter for plotting validation losses       \n",
    "             \n",
    "            if batch_idx == 1000:    # tell your model to stop at some batch_idx if you want\n",
    "                #pass\n",
    "                break\n",
    "        \n",
    "        # VALIDATING\n",
    "        print('Validating')\n",
    "        val_losses = []\n",
    "        validation_loss_idxs.append(batch_idx_for_val)\n",
    "        for batch in val_loader:\n",
    "            features, labels = batch                   # unpack the batch\n",
    "            prediction = model(features)               # pass an example's features forward through the model\n",
    "            val_loss = criterion(prediction, labels)\n",
    "            val_losses.append(val_loss.item())\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        validation_losses.append(avg_val_loss)\n",
    "               \n",
    "    return train_losses, validation_loss_idxs, val_losses    # return the list of losses from the training function\n",
    "            \n",
    "train_loss_list, val_idxs, val_loss_list = train(nn)   # call the training function and store the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)     # plot the training losses\n",
    "plt.plot(val_idxs, val_loss_list)\n",
    "plt.show()     # show the training curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
